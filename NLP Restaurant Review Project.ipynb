{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              Review|Liked|\n",
      "+--------------------+-----+\n",
      "|Wow... Loved this...|    1|\n",
      "|  Crust is not good.|    0|\n",
      "|Not tasty and the...|    0|\n",
      "|Stopped by during...|    1|\n",
      "|The selection on ...|    1|\n",
      "|Now I am getting ...|    0|\n",
      "|Honeslty it didn'...|    0|\n",
      "|The potatoes were...|    0|\n",
      "|The fries were gr...|    1|\n",
      "|      A great touch.|    1|\n",
      "|Service was very ...|    1|\n",
      "|  Would not go back.|    0|\n",
      "|The cashier had n...|    0|\n",
      "|I tried the Cape ...|    1|\n",
      "|I was disgusted b...|    0|\n",
      "|I was shocked bec...|    0|\n",
      "| Highly recommended.|    1|\n",
      "|Waitress was a li...|    0|\n",
      "|This place is not...|    0|\n",
      "|did not like at all.|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('directory_to_spark_installation')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "data = spark.read.csv('Restaurant_Reviews.tsv',\n",
    "                      inferSchema = True, \n",
    "                      header = True, # no header defined in the data\n",
    "                     sep = '\\t' # since the document is tab-separated\n",
    "                     )\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|              Review|Liked|Length|\n",
      "+--------------------+-----+------+\n",
      "|Wow... Loved this...|    1|    24|\n",
      "|  Crust is not good.|    0|    18|\n",
      "|Not tasty and the...|    0|    41|\n",
      "|Stopped by during...|    1|    87|\n",
      "|The selection on ...|    1|    59|\n",
      "|Now I am getting ...|    0|    46|\n",
      "|Honeslty it didn'...|    0|    37|\n",
      "|The potatoes were...|    0|   111|\n",
      "|The fries were gr...|    1|    25|\n",
      "|      A great touch.|    1|    14|\n",
      "|Service was very ...|    1|    24|\n",
      "|  Would not go back.|    0|    18|\n",
      "|The cashier had n...|    0|    99|\n",
      "|I tried the Cape ...|    1|    60|\n",
      "|I was disgusted b...|    0|    62|\n",
      "|I was shocked bec...|    0|    50|\n",
      "| Highly recommended.|    1|    19|\n",
      "|Waitress was a li...|    0|    38|\n",
      "|This place is not...|    0|    51|\n",
      "|did not like at all.|    0|    20|\n",
      "+--------------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "data = data.withColumn('Length', length(data['Review']))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|Liked|avg(Liked)|avg(length)|\n",
      "+-----+----------+-----------+\n",
      "|    1|       1.0|      55.88|\n",
      "|    0|       0.0|      60.75|\n",
      "+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('Liked').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** This shows that simply the length of the review does not tell whether a customer like the restaurant or not ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Moving Forward ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------+\n",
      "|Liked|Clean_Review                                                                                                   |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------+\n",
      "|1    |wow    loved this place                                                                                        |\n",
      "|0    |crust is not good                                                                                              |\n",
      "|0    |not tasty and the texture was just nasty                                                                       |\n",
      "|1    |stopped by during the late may bank holiday off rick steve recommendation and loved it                         |\n",
      "|1    |the selection on the menu was great and so were the prices                                                     |\n",
      "|0    |now i am getting angry and i want my damn pho                                                                  |\n",
      "|0    |honeslty it didn t taste that fresh                                                                            |\n",
      "|0    |the potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer |\n",
      "|1    |the fries were great too                                                                                       |\n",
      "|1    |a great touch                                                                                                  |\n",
      "|1    |service was very prompt                                                                                        |\n",
      "|0    |would not go back                                                                                              |\n",
      "|0    |the cashier had no care what so ever on what i had to say it still ended up being wayyy overpriced             |\n",
      "|1    |i tried the cape cod ravoli  chicken  with cranberry   mmmm                                                    |\n",
      "|0    |i was disgusted because i was pretty sure that was human hair                                                  |\n",
      "|0    |i was shocked because no signs indicate cash only                                                              |\n",
      "|1    |highly recommended                                                                                             |\n",
      "|0    |waitress was a little slow in service                                                                          |\n",
      "|0    |this place is not worth your time  let alone vegas                                                             |\n",
      "|0    |did not like at all                                                                                            |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuations and converting to lowercase\n",
    "from pyspark.sql.functions import regexp_replace, lower\n",
    "\n",
    "clean_data = data.select('Liked', lower(regexp_replace('Review', '[^a-zA-Z]', ' ')).alias('Clean_Review'))\n",
    "clean_data.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'Clean_Review', outputCol = 'tokenized_review')\n",
    "stop_remove = StopWordsRemover(inputCol = 'tokenized_review', outputCol = 'stop_review')\n",
    "count_vec = CountVectorizer(inputCol = 'stop_review', outputCol = 'vectorized_review')\n",
    "idf = IDF(inputCol = 'vectorized_review', outputCol = 'idf_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['idf_review'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(1891,[0,2,91,369...|\n",
      "|    0|(1891,[3,511],[2....|\n",
      "|    0|(1891,[81,375,447...|\n",
      "|    1|(1891,[91,220,338...|\n",
      "|    1|(1891,[5,49,97,98...|\n",
      "|    0|(1891,[59,133,176...|\n",
      "|    0|(1891,[58,62,1726...|\n",
      "|    0|(1891,[7,9,41,44,...|\n",
      "|    1|(1891,[5,103],[2....|\n",
      "|    1|(1891,[5,600],[2....|\n",
      "|    1|(1891,[4,1058],[2...|\n",
      "|    0|(1891,[6,8,12],[2...|\n",
      "|    0|(1891,[13,42,74,1...|\n",
      "|    1|(1891,[0,40,163,5...|\n",
      "|    0|(1891,[24,130,497...|\n",
      "|    0|(1891,[1015,1033,...|\n",
      "|    1|(1891,[295,355],[...|\n",
      "|    0|(1891,[4,73,86,11...|\n",
      "|    0|(1891,[0,2,9,22,8...|\n",
      "|    0|(1891,[7],[3.1020...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a pipeline with all the objects defined above\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_remove, count_vec, idf, assembler])\n",
    "final_data = pipeline.fit(clean_data).transform(clean_data)\n",
    "final_data = final_data.select(['Liked', 'features'])\n",
    "final_data = final_data.withColumnRenamed('Liked', 'label')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|(1891,[0,1,2,4,56...|[-210.84794317662...|[0.99999999999989...|       0.0|\n",
      "|    0|(1891,[0,1,4,73,8...|[-341.30044283886...|[1.0,2.0656000869...|       0.0|\n",
      "|    0|(1891,[0,1,7,18,2...|[-505.22567187105...|[1.0,3.8578479560...|       0.0|\n",
      "|    0|(1891,[0,1,10,45,...|[-218.88000634057...|[1.54057614300251...|       1.0|\n",
      "|    0|(1891,[0,1,31,57,...|[-154.63041719070...|[0.99981339687064...|       0.0|\n",
      "|    0|(1891,[0,1,33,35,...|[-568.62079875290...|[0.99996648951684...|       0.0|\n",
      "|    0|(1891,[0,1,33,60,...|[-299.85607601610...|[1.0,2.7042994614...|       0.0|\n",
      "|    0|(1891,[0,1,44,282...|[-224.67092040834...|[0.99999999672894...|       0.0|\n",
      "|    0|(1891,[0,2,6,75,1...|[-195.75889691146...|[0.98874269190624...|       0.0|\n",
      "|    0|(1891,[0,2,7,10,3...|[-303.32007192321...|[0.99999997682913...|       0.0|\n",
      "|    0|(1891,[0,2,9,39,1...|[-323.76965498278...|[8.93831386880982...|       1.0|\n",
      "|    0|(1891,[0,2,12,41,...|[-408.89491393045...|[0.45137081747301...|       1.0|\n",
      "|    0|(1891,[0,2,313,68...|[-232.62131089940...|[9.65588433754607...|       1.0|\n",
      "|    0|(1891,[0,3,7,25,8...|[-530.91931213943...|[0.99966970101532...|       0.0|\n",
      "|    0|(1891,[0,3,278],[...|[-53.131772295767...|[0.99999211686453...|       0.0|\n",
      "|    0|(1891,[0,4,37,61,...|[-110.10166289286...|[0.99999999863195...|       0.0|\n",
      "|    0|(1891,[0,4,43,127...|[-575.33515187012...|[4.85043063763224...|       1.0|\n",
      "|    0|(1891,[0,4,1559,1...|[-134.98408772124...|[0.18363731103104...|       1.0|\n",
      "|    0|(1891,[0,6,8,13,1...|[-73.863397187398...|[0.99999844875651...|       0.0|\n",
      "|    0|(1891,[0,6,8,43,5...|[-115.52675232319...|[0.01002512025585...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes model is very common to use with NLP\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# Splitting the data into train set and test set\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Fitting the classifier model\n",
    "nb = NaiveBayes()\n",
    "nbModel = nb.fit(train_data)\n",
    "\n",
    "# Making prediction using text data and trained model\n",
    "result = nbModel.transform(test_data)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.7046917595293892\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate othe performance of our model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "eval1 = MulticlassClassificationEvaluator()\n",
    "accuracy = eval1.evaluate(result)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
