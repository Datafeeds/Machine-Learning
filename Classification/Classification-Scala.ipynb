{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.137.1:4040\n",
       "SparkContext available as 'sc' (version = 2.2.0, master = local[*], app id = local-1501538400462)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6e331193\r\n",
       "data: org.apache.spark.sql.DataFrame = [age: int, job: string ... 15 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "val data = (spark.read.option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .format(\"csv\")\n",
    "            .option(\"sep\", \";\")\n",
    "            .load(\"C:/Users/ibhat/Desktop/Projects/Bank_Marketing/bank.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome, y)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "| 58|  management| married| tertiary|     no|   2143|    yes|  no|unknown|  5|  may|     261|       1|   -1|       0| unknown| no|\n",
      "| 44|  technician|  single|secondary|     no|     29|    yes|  no|unknown|  5|  may|     151|       1|   -1|       0| unknown| no|\n",
      "| 33|entrepreneur| married|secondary|     no|      2|    yes| yes|unknown|  5|  may|      76|       1|   -1|       0| unknown| no|\n",
      "| 47| blue-collar| married|  unknown|     no|   1506|    yes|  no|unknown|  5|  may|      92|       1|   -1|       0| unknown| no|\n",
      "| 33|     unknown|  single|  unknown|     no|      1|     no|  no|unknown|  5|  may|     198|       1|   -1|       0| unknown| no|\n",
      "| 35|  management| married| tertiary|     no|    231|    yes|  no|unknown|  5|  may|     139|       1|   -1|       0| unknown| no|\n",
      "| 28|  management|  single| tertiary|     no|    447|    yes| yes|unknown|  5|  may|     217|       1|   -1|       0| unknown| no|\n",
      "| 42|entrepreneur|divorced| tertiary|    yes|      2|    yes|  no|unknown|  5|  may|     380|       1|   -1|       0| unknown| no|\n",
      "| 58|     retired| married|  primary|     no|    121|    yes|  no|unknown|  5|  may|      50|       1|   -1|       0| unknown| no|\n",
      "| 43|  technician|  single|secondary|     no|    593|    yes|  no|unknown|  5|  may|      55|       1|   -1|       0| unknown| no|\n",
      "| 41|       admin|divorced|secondary|     no|    270|    yes|  no|unknown|  5|  may|     222|       1|   -1|       0| unknown| no|\n",
      "| 29|       admin|  single|secondary|     no|    390|    yes|  no|unknown|  5|  may|     137|       1|   -1|       0| unknown| no|\n",
      "| 53|  technician| married|secondary|     no|      6|    yes|  no|unknown|  5|  may|     517|       1|   -1|       0| unknown| no|\n",
      "| 58|  technician| married|  unknown|     no|     71|    yes|  no|unknown|  5|  may|      71|       1|   -1|       0| unknown| no|\n",
      "| 57|    services| married|secondary|     no|    162|    yes|  no|unknown|  5|  may|     174|       1|   -1|       0| unknown| no|\n",
      "| 51|     retired| married|  primary|     no|    229|    yes|  no|unknown|  5|  may|     353|       1|   -1|       0| unknown| no|\n",
      "| 45|       admin|  single|  unknown|     no|     13|    yes|  no|unknown|  5|  may|      98|       1|   -1|       0| unknown| no|\n",
      "| 57| blue-collar| married|  primary|     no|     52|    yes|  no|unknown|  5|  may|      38|       1|   -1|       0| unknown| no|\n",
      "| 60|     retired| married|  primary|     no|     60|    yes|  no|unknown|  5|  may|     219|       1|   -1|       0| unknown| no|\n",
      "| 33|    services| married|secondary|     no|      0|    yes|  no|unknown|  5|  may|      54|       1|   -1|       0| unknown| no|\n",
      "+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All feature \n",
    "age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome\n",
    "\n",
    "##### Features being Considered\n",
    "age, job, marital,  default, balance, housing, loan, duration, campaign, pdays, previous, poutcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|poutcome|count|\n",
      "+--------+-----+\n",
      "| success| 1511|\n",
      "| unknown|36959|\n",
      "|   other| 1840|\n",
      "| failure| 4901|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"poutcome\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'poutcome' seems to be important feature. However, the previous outcome was unknown for majority of the customers. So, It is better to drop this feature.\n",
    "\n",
    "##### Final Features being Considered\n",
    "age, job, marital,  default, balance, housing, loan, duration, campaign, pdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+--------+-------+------------------+-------+-----+-----------------+-----------------+------------------+-----+\n",
      "|summary|               age|    job| marital|default|           balance|housing| loan|         duration|         campaign|             pdays|    y|\n",
      "+-------+------------------+-------+--------+-------+------------------+-------+-----+-----------------+-----------------+------------------+-----+\n",
      "|  count|             45211|  45211|   45211|  45211|             45211|  45211|45211|            45211|            45211|             45211|45211|\n",
      "|   mean| 40.93621021432837|   null|    null|   null|1362.2720576850766|   null| null|258.1630797814691|2.763840658246887| 40.19782796222158| null|\n",
      "| stddev|10.618762040975405|   null|    null|   null|3044.7658291685257|   null| null|257.5278122651706|3.098020883279192|100.12874599059828| null|\n",
      "|    min|                18|  admin|divorced|     no|             -8019|     no|   no|                0|                1|                -1|   no|\n",
      "|    max|                95|unknown|  single|    yes|            102127|    yes|  yes|             4918|               63|               871|  yes|\n",
      "+-------+------------------+-------+--------+-------+------------------+-------+-----+-----------------+-----------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "final_data: org.apache.spark.sql.DataFrame = [age: int, job: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_data = data.select(\"age\", \"job\", \"marital\", \"default\", \"balance\", \"housing\", \"loan\", \"duration\", \"campaign\", \"pdays\", \"y\")\n",
    "final_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no null values in the columns being considered. Good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoder}\r\n",
       "JobIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_48eff509bb4a\r\n",
       "MaritalIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_7eb1612fb40b\r\n",
       "DefaultIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_fe91b1088f88\r\n",
       "HousingIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_a63c3c892fbf\r\n",
       "LoanIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_6d0bab2d72a9\r\n",
       "OutcomeIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_85b75a1400c5\r\n",
       "JobEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_0a2ad65186d7\r\n",
       "MaritalEncoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_3541bf8bfe6a\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoder}\n",
    "\n",
    "\n",
    "// Converting string values to numerical column\n",
    "val JobIndexer = new StringIndexer().setInputCol(\"job\").setOutputCol(\"JobIndex\")\n",
    "val MaritalIndexer = new StringIndexer().setInputCol(\"marital\").setOutputCol(\"MaritalIndex\")\n",
    "val DefaultIndexer = new StringIndexer().setInputCol(\"default\").setOutputCol(\"DefaultIndex\")\n",
    "val HousingIndexer = new StringIndexer().setInputCol(\"housing\").setOutputCol(\"HousingIndex\")\n",
    "val LoanIndexer = new StringIndexer().setInputCol(\"loan\").setOutputCol(\"LoanIndex\")\n",
    "val OutcomeIndexer = new StringIndexer().setInputCol(\"y\").setOutputCol(\"label\")\n",
    "\n",
    "// Using OneHotEncoder to avoid hierarchy in numerical value obtaied in above step\n",
    "val JobEncoder = new OneHotEncoder().setInputCol(\"JobIndex\").setOutputCol(\"JobVec\")\n",
    "val MaritalEncoder = new OneHotEncoder().setInputCol(\"MaritalIndex\").setOutputCol(\"MaritalkVec\")\n",
    "\n",
    "// All the other columns have binary values(either Yes or No). So no need to hot-encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4c0cab1c7ca3\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assemble everything together to be (\"label\",\"features\") format\n",
    "val assembler = (new VectorAssembler()\n",
    "                .setInputCols(Array(\"age\", \"JobVec\", \"MaritalkVec\", \"DefaultIndex\", \"balance\", \n",
    "                                    \"HousingIndex\", \"LoanIndex\", \"duration\", \"campaign\", \"pdays\"))\n",
    "                .setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StandardScaler\r\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_7718459a2d97\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Scaling the features\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = (new StandardScaler()\n",
    "              .setInputCol(\"features\")\n",
    "              .setOutputCol(\"scaledFeatures\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [age: int, job: string ... 9 more fields]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [age: int, job: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Spliting the data into Training set and Test set\n",
    "val Array(training, test) = final_data.randomSplit(Array(0.7, 0.3), seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LogisticRegression, RandomForestClassifier}\r\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_bd73315b1264\r\n",
       "rcf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_586ace129f3e\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Defining Classifier Model\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, RandomForestClassifier}\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "val rcf = new RandomForestClassifier().setMaxDepth(10).setNumTrees(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\r\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_cb6478b8b9f6\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set Up the Pipeline\n",
    "import org.apache.spark.ml.Pipeline\n",
    "// This creates the stages for the task we were trying to accomplish in last few stages before\n",
    "\n",
    "// *************************** For Logistic Regressor ***************************\n",
    "// val pipeline = new Pipeline().setStages(Array(JobIndexer, MaritalIndexer, DefaultIndexer, HousingIndexer,\n",
    "//                                               LoanIndexer, OutcomeIndexer, JobEncoder, MaritalEncoder, assembler, scaler, lr))\n",
    "\n",
    "// *************************** For Random Forest Classifier ***************************\n",
    "val pipeline = new Pipeline().setStages(Array(JobIndexer, MaritalIndexer, DefaultIndexer, HousingIndexer,\n",
    "                                              LoanIndexer, OutcomeIndexer, JobEncoder, MaritalEncoder, assembler, scaler, rcf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_cb6478b8b9f6\r\n",
       "results: org.apache.spark.sql.DataFrame = [age: int, job: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fitting the model\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Geting results on Test set\n",
    "val results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       1.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.MulticlassMetrics\r\n",
       "predictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[351] at rdd at <console>:64\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Model Evaluation\n",
    "\n",
    "// Model selection is still in RDD phase. So We will use rdd here instead of spark dataframe. \n",
    "// This will be update in future versions of Spark\n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
    "\n",
    "val predictionAndLabels = results.select($\"prediction\",$\"label\").as[(Double, Double)].rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "11932.0  191.0  \n",
      "1258.0   376.0  \n",
      "Accuracy: 0.8946718034455187\n",
      "Precision : 0.875942072636317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@105bbc9f\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "// Confusion Matrix\n",
    "println(\"Confusion Matrix\")\n",
    "println(metrics.confusionMatrix)\n",
    "println(s\"Accuracy: ${metrics.accuracy}\")\n",
    "println(s\"Precision : ${metrics.weightedPrecision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
