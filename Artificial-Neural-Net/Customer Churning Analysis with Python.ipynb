{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('Bank Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
    
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Since we have some categorical variable in ur dataset, we need to map them to numerical values for ML processing\n",
    "# Encoding the categorical variable\n",
    "# Encoding the Independent Variable(For Geography and Gender)\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "# In order to make sure there is no hierarchy for categorical variables\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "# This avoids dummy variable trap(for n category, we just need n-1 dummy variable)\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split # cross_validation is replaced by model_selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n""  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4774 - acc: 0.7959     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4278 - acc: 0.7960     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4227 - acc: 0.8019     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4183 - acc: 0.8227     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4163 - acc: 0.8259     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4141 - acc: 0.8296     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4127 - acc: 0.8325     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4111 - acc: 0.8332     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4103 - acc: 0.8332     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4096 - acc: 0.8342     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4086 - acc: 0.8339     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4079 - acc: 0.8337     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4071 - acc: 0.8344     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4066 - acc: 0.8339     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4061 - acc: 0.8349     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4059 - acc: 0.8354     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4052 - acc: 0.8350     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4042 - acc: 0.8354     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4045 - acc: 0.8337     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4037 - acc: 0.8359     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4040 - acc: 0.8351     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4035 - acc: 0.8351     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4031 - acc: 0.8341     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4035 - acc: 0.8342     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4033 - acc: 0.8346     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4031 - acc: 0.8344     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4027 - acc: 0.8347     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4027 - acc: 0.8352     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4021 - acc: 0.8341     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4024 - acc: 0.8349     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4020 - acc: 0.8355     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4020 - acc: 0.8345     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4023 - acc: 0.8359     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4016 - acc: 0.8341     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4022 - acc: 0.8354     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4017 - acc: 0.8347     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4015 - acc: 0.8340     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4018 - acc: 0.8351     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8351     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4020 - acc: 0.8347     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4009 - acc: 0.8342     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4015 - acc: 0.8351     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4012 - acc: 0.8341     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4007 - acc: 0.8349     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4012 - acc: 0.8355     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4014 - acc: 0.8356     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4013 - acc: 0.8351     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8355     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4014 - acc: 0.8351     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4012 - acc: 0.8355     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4008 - acc: 0.8339     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8341     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4011 - acc: 0.8357     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4011 - acc: 0.8347     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8352     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4009 - acc: 0.8342     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8355     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4003 - acc: 0.8357     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4011 - acc: 0.8337     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8345     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8354     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8351     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4003 - acc: 0.8339     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8345     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8355     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8345     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8329     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4007 - acc: 0.8354     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8336     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8361     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4008 - acc: 0.8354     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4004 - acc: 0.8347     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3999 - acc: 0.8345     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8349     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4003 - acc: 0.8355     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8331     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4006 - acc: 0.8346     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4000 - acc: 0.8345     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4001 - acc: 0.8337     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4005 - acc: 0.8354     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3998 - acc: 0.8345     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4000 - acc: 0.8364     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4000 - acc: 0.8354     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4004 - acc: 0.8350     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.3999 - acc: 0.8359     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.3999 - acc: 0.8359     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.3999 - acc: 0.8359     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4004 - acc: 0.8357     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4002 - acc: 0.8347     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.3998 - acc: 0.8339     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3992 - acc: 0.8357     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4002 - acc: 0.8357     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.3999 - acc: 0.8351     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4003 - acc: 0.8359     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4006 - acc: 0.8351     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3999 - acc: 0.8340     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.3999 - acc: 0.8355     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3998 - acc: 0.8352     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3992 - acc: 0.8347     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4000 - acc: 0.8359     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21df257ab38>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initializing the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "# we will have 11 input nodes(for 11 independent variables)\n",
    "# The best activation function could be rectifier function for the hidden layer and sigmoid function for the output layer\n",
    "# We will also be able to find the rank of probability that customer could leave the bank\n",
    "classifier.add(Dense(output_dim = 8,# number of nodes in the hidden layer being added. Usual practice is to take an average of number of layers in i/p layer and o/p layers. Or performance tuning by K-fold cross validation \n",
    "init = 'uniform' , # initialize the weights to small number close to 0\n",
    "activation = 'relu', # rectifier activation function for hidden layer\n",
    "input_dim = 11 # number of nodes in th i/p layer( # of independent variable)\n",
    ")) # all the NN parameters are defined here\n",
    "\n",
    "# Adding additional hidden layer\n",
    "classifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'relu')) # input_dim is required only for first hidden layer as the NN model does not know how many nodes are at the input but after first hidden layet, the model knows how many input are there in the following hidden layers\n",
    "\n",
    "# Adding additional hidden layer\n",
    "classifier.add(Dense(output_dim = 4, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, # since we are expecting binary output, we need 1 output node. \n",
    "init = 'uniform', \n",
    "activation = 'sigmoid' # sigmoid activation function for output layer\n",
    "# for muti-value output categories, we need output_dim = nimber of categories and activation = 'soft_max'\n",
    "# soft_max is similar to sigmoid function but applied to dependent varibale that has more than 2 categories\n",
    ")) \n",
    "\n",
    "# Compiling the ANN\n",
    "# Applying stochastic gradient descent in the entire NN\n",
    "classifier.compile(\n",
    "optimizer = 'adam', # algorithm to find optimal set of weights for NN\n",
    "loss = 'binary_crossentropy', # loss function within the stochastic gradient descent algorithm (i.e. in 'adam' algorithm\n",
    "# binary_crossentropy -> for binary o/p and categorical_crossentropy -> for categorical o/p\n",
    "metrics = ['accuracy'] # accuracy criterion to evaluate the model\n",
    ")\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, \n",
    "batch_size = 10, # whether to update the weights after each observation or after a batch of obobservation ckpropagation\n",
    "nb_epoch = 100 # defines number of iterations\n",
    "# for both batch_size and nb_epoch, no optimal value by default. Need to find the best value by experimentation or performance tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1525,   70],\n",
       "       [ 247,  158]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# since the y_pred here is the probabilities instead of binary value, we need to convert these probabilities into a binary value. For this weed to set a threshold to distinguish between 1 and 0.\n",
    "# for sensitive information we need higher threshold\n",
    "# let's choose 50% as the threshold here\n",
    "y_pred = (y_pred > 0.5) # this gives true/false\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8415\n",
      "0.69298245614\n"
     ]
    }
   ],
   "source": [
    "# Computing the Test Accuracy\n",
    "test_accuracy = (cm[1,1] + cm[0,0]) / sum(sum(cm))\n",
    "print(test_accuracy)\n",
    "# Computing the Test Precision\n",
    "test_precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "print(test_precision)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
