{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.6:4042\n",
       "SparkContext available as 'sc' (version = 2.2.0, master = local[*], app id = local-1501628004848)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if that?s th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3ccb909b\r\n",
       "data: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\r\n",
       "new_data: org.apache.spark.sql.DataFrame = [class: string, text: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "val data = (spark.read.option(\"header\", \"false\") // no header in the data\n",
    "            .option(\"inferSchema\", \"True\")\n",
    "            .option(\"sep\", \"\\t\")\n",
    "            .csv(\"SMSSpamCollection\"))\n",
    "\n",
    "// since the data does not have header, let's define column name\n",
    "val new_data = data.withColumnRenamed(\"_c0\", \"class\").withColumnRenamed(\"_c1\", \"text\")\n",
    "new_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|                text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if that?s th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.length\r\n",
       "len_data: org.apache.spark.sql.DataFrame = [class: string, text: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.length\n",
    "var len_data = new_data.withColumn(\"length\", length($\"text\"))\n",
    "len_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "len_data.groupBy(\"class\").mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** This shows that the length of the spam test is almost double the ham test. So the length could be a good feature to consider***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|          Clean_text|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|go until jurong p...|   111|\n",
      "|  ham|ok lar    joking ...|    29|\n",
      "| spam|free entry in   a...|   155|\n",
      "|  ham|u dun say so earl...|    49|\n",
      "|  ham|nah i don t think...|    61|\n",
      "| spam|freemsg hey there...|   147|\n",
      "|  ham|even my brother i...|    77|\n",
      "|  ham|as per your reque...|   160|\n",
      "| spam|winner   as a val...|   157|\n",
      "| spam|had your mobile  ...|   154|\n",
      "|  ham|i m gonna be home...|   109|\n",
      "| spam|six chances to wi...|   136|\n",
      "| spam|urgent  you have ...|   155|\n",
      "|  ham|i ve been searchi...|   196|\n",
      "|  ham|i have a date on ...|    35|\n",
      "| spam|xxxmobilemovieclu...|   149|\n",
      "|  ham|oh k   i m watchi...|    26|\n",
      "|  ham|eh u remember how...|    81|\n",
      "|  ham|fine if that s th...|    56|\n",
      "| spam|england v macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{regexp_replace, lower}\r\n",
       "clean_data: org.apache.spark.sql.DataFrame = [class: string, Clean_text: string ... 1 more field]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{regexp_replace, lower}\n",
    "val clean_data = len_data.select($\"class\", lower(regexp_replace(col(\"text\"), \"[^a-zA-Z]\", \" \")).as(\"Clean_text\"), $\"length\")\n",
    "clean_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer}\r\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_017d08f93c25\r\n",
       "stop_remove: org.apache.spark.ml.feature.StopWordsRemover = stopWords_74c79c0c369b\r\n",
       "count_vec: org.apache.spark.ml.feature.CountVectorizer = cntVec_5b8e5281c542\r\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_30925198af5d\r\n",
       "class_label: org.apache.spark.ml.feature.StringIndexer = strIdx_1925cf98ec24\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer}\n",
    "val tokenizer = new Tokenizer().setInputCol(\"Clean_text\").setOutputCol(\"token_text\")\n",
    "val stop_remove = new StopWordsRemover().setInputCol(\"token_text\").setOutputCol(\"stop_token\")\n",
    "val count_vec = new CountVectorizer().setInputCol(\"stop_token\").setOutputCol (\"c_vec\")\n",
    "val idf = new IDF().setInputCol(\"c_vec\").setOutputCol(\"tf_idf\")\n",
    "val class_label = new StringIndexer().setInputCol(\"class\").setOutputCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_339528a5ac9e\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = (new VectorAssembler()\n",
    "                .setInputCols(Array(\"tf_idf\", \"length\"))\n",
    "                .setOutputCol(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(7662,[0,9,16,25,...|\n",
      "|  0.0|(7662,[0,1,8,248,...|\n",
      "|  1.0|(7662,[0,10,22,24...|\n",
      "|  0.0|(7662,[0,1,52,80,...|\n",
      "|  0.0|(7662,[0,51,136,3...|\n",
      "|  1.0|(7662,[0,8,14,20,...|\n",
      "|  0.0|(7662,[0,14,127,2...|\n",
      "|  0.0|(7662,[0,145,158,...|\n",
      "|  1.0|(7662,[0,2,64,81,...|\n",
      "|  1.0|(7662,[0,1,2,10,2...|\n",
      "|  0.0|(7662,[0,3,21,30,...|\n",
      "|  1.0|(7662,[0,15,20,23...|\n",
      "|  1.0|(7662,[0,10,24,52...|\n",
      "|  0.0|(7662,[0,44,75,83...|\n",
      "|  0.0|(7662,[477,657,76...|\n",
      "|  1.0|(7662,[0,24,25,78...|\n",
      "|  0.0|(7662,[0,3,35,61,...|\n",
      "|  0.0|(7662,[0,1,70,72,...|\n",
      "|  0.0|(7662,[0,1,66,71,...|\n",
      "|  1.0|(7662,[0,5,24,41,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\r\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_d6cd1f5dc1dc\r\n",
       "final_data: org.apache.spark.sql.DataFrame = [class: string, Clean_text: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Let's create a pipeline with all the objects defined above\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(class_label, tokenizer, stop_remove, count_vec, idf, assembler))\n",
    "val final_data = pipeline.fit(clean_data).transform(clean_data)\n",
    "final_data.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(7662,[0,1,2,8,32...|[-147.25744679125...|[0.99999999965130...|       0.0|\n",
      "|  0.0|(7662,[0,1,2,8,74...|[-166.15149758450...|[0.14452257918736...|       1.0|\n",
      "|  0.0|(7662,[0,1,2,10,2...|[-487.73019302991...|[1.0,2.3175437907...|       0.0|\n",
      "|  0.0|(7662,[0,1,2,13,1...|[-588.88957009481...|[1.0,6.4204232555...|       0.0|\n",
      "|  0.0|(7662,[0,1,2,24,3...|[-416.47617533368...|[0.99999999999980...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,4,5,...|[-2614.0422941739...|[1.0,1.4961496857...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,8,9,...|[-840.51123980113...|[1.0,3.7805434265...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,9,12...|[-1005.1979496043...|[1.0,3.5991093813...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,12,5...|[-577.74435276786...|[1.0,2.1593665931...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,26,9...|[-435.32287975397...|[1.0,2.1985980981...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,27,2...|[-505.35506581546...|[1.0,3.9022411776...|       0.0|\n",
      "|  0.0|(7662,[0,1,3,27,1...|[-398.08975479387...|[1.0,1.3965847609...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,5,14...|[-1575.3415791721...|[1.0,2.1160218022...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,6,7,...|[-2130.9108622970...|[1.0,1.6655213468...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,8,25...|[-1040.3133463876...|[1.0,5.1898486683...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,11,1...|[-953.94620735110...|[1.0,2.2838012255...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,14,1...|[-768.34282161178...|[1.0,2.7832586909...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,25,2...|[-2315.8502003649...|[1.0,1.7302486537...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,28,3...|[-1200.0457198051...|[1.0,5.1792831135...|       0.0|\n",
      "|  0.0|(7662,[0,1,4,28,4...|[-918.47436108662...|[1.0,2.0196445786...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.NaiveBayes\r\n",
       "train_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "test_data: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\r\n",
       "nb: org.apache.spark.ml.classification.NaiveBayes = nb_e177e974ed95\r\n",
       "nbModel: org.apache.spark.ml.classification.NaiveBayesModel = NaiveBayesModel (uid=nb_e177e974ed95) with 2 classes\r\n",
       "result: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Naive bayes model is very common to use with NLP\n",
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "\n",
    "// Splitting the data into train set and test set\n",
    "val Array(train_data, test_data) = final_data.select(\"label\", \"features\").randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "// Fitting the classifier model\n",
    "val nb = new NaiveBayes()\n",
    "val nbModel = nb.fit(train_data)\n",
    "\n",
    "// Making prediction using text data and trained model\n",
    "val result = nbModel.transform(test_data)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.9496410835752409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\r\n",
       "eval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_c5dba066543f\r\n",
       "accuracy: Double = 0.9496410835752409\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// let's evaluate othe performance of our model\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val eval = new MulticlassClassificationEvaluator()\n",
    "val accuracy = eval.evaluate(result)\n",
    "println($\"Accuracy of model at predicting spam was: $accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
